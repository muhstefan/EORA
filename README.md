# Бот о компании EORA
Проект предоставляет возможность интерактивного поиска по набору текстовых данных, используя предварительную обработку текста (токенизацию, лемматизацию, удаление стоп-слов), построение TF-IDF индекса и интеграцию с чат-ботом GigaChat для генерации ответов с источниками.

## Быстрый запуск:
создайте модуль private.py где нужно указать переменную authorize_key (это секретный токен, поэтому в репозитории его нет)

pip install poetry

poetry install

poetry run python main.py


Подождите пока данные с сайтов будут получены, и после сообщение "Введите запрос" вводите запрос)

### Завершение работы:
По вводу q программа корректно закрывает сессии чат-бота.

## Особенности и идеи решения 
(тут краткое содержание по пунктам: что пробовали сделать  + что бы ещё добавили в решение, если бы было больше времени.)

-Чтение исходного файла с ссылками на ресурс.

-Асинхронный сбор данных с сайтов (асинхронный клиент).

-Разбиение текста на смысловые блоки с последующей лемматизацией с фильтрацией шумовой информации и очисткой от лишнего.

-Создание TF-IDF индекса для быстрого поиска релевантных текстовых блоков.

(Тут можно было бы использовать индексы блоков, сделав указание на место в тексте, + использовать более сложную модель построения индексов например Inverted Index)

-Выборка наиболее релевантных блоков с анализом пересечения лемм запроса и текста.

(Можно проанализировать и улучшить алгоритмы поиска)

-Интеграция с GigaChat для генерации информативных ответов на вопросы пользователя с указанием источников.

(Можно было бы обернуть это в бота или API, оценить мои возможно построения API можно по проекту GameScore
https://github.com/muhstefan/GameScore )


## Установка зависимостей
За установку модулей отвечает poetry
Python 3.8+

"gigachat (>=0.1.42.post1,<0.2.0)",
"bs4 (>=0.0.2,<0.0.3)",
"nltk (>=3.9.1,<4.0.0)",
"aiohttp (>=3.12.15,<4.0.0)",
"joblib (>=1.5.1,<2.0.0)",
"spacy (>=3.8.7,<4.0.0)",
"scikit-learn (>=1.7.1,<2.0.0)"


## Основные модули
main.py — организация процесса: чтение ссылок, запуск парсинга, фильтрация, сохранение и интерфейс запроса.

nlp_module.py — обработка текста, лемматизация, построение TF-IDF и поиск релевантных блоков.

giga.py — обертка для работы с GigaChat API, формирование контекста и получение ответов.

MyLibs\data Классы для хранения, самое важное там описание CompositeData, это синглтон класс для получения данных с сайтов.
MyLibs\aioclient Асинхронный клиент для парсинга
MyLibs\processor описание стратегии для парсинга данных


## Оценка работы
Хорошо получилось распарсить все страницы, неплохо вышло очистить и привести к общему виду данные, при условии что страницы делились по разному. Так же неплохо все это работает вместе.
Улучшить можно было бы это добавить интеграцию с API \ ботом, чтобы это был законченный продукт. Можно было подключить БД если бы это был Продакшн и больший объём данных. И можно было бы больше углубится в отлов краевых значений , более тонких ошибок, и рефакторинг. Конкретно можно было бы разбить еще функции чтобы много бизнес логики не было
Оцениваю решение на 7\10 , любой код можно улучшить и этот не исключение,

***Благодарю за внимание)***
